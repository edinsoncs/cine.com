{
  "_args": [
    [
      "crawler@^0.4.3",
      "D:\\proyectos\\crawlers\\crawler"
    ]
  ],
  "_from": "crawler@>=0.4.3 <0.5.0",
  "_id": "crawler@0.4.3",
  "_inCache": true,
  "_installable": true,
  "_location": "/crawler",
  "_nodeVersion": "0.12.0",
  "_npmUser": {
    "email": "bonjour@pol.ninja",
    "name": "paulvalla"
  },
  "_npmVersion": "2.5.1",
  "_phantomChildren": {},
  "_requested": {
    "name": "crawler",
    "raw": "crawler@^0.4.3",
    "rawSpec": "^0.4.3",
    "scope": null,
    "spec": ">=0.4.3 <0.5.0",
    "type": "range"
  },
  "_requiredBy": [
    "/"
  ],
  "_resolved": "https://registry.npmjs.org/crawler/-/crawler-0.4.3.tgz",
  "_shasum": "15a7f65b68ab2e55613254490c94d7994a4e2c59",
  "_shrinkwrap": null,
  "_spec": "crawler@^0.4.3",
  "_where": "D:\\proyectos\\crawlers\\crawler",
  "bugs": {
    "url": "http://github.com/sylvinus/node-crawler/issues"
  },
  "dependencies": {
    "cheerio": "0.18.0",
    "generic-pool": "2.1.1",
    "iconv": "*",
    "iconv-lite": "0.4.4",
    "jschardet": "1.1.0",
    "lodash": "2.4.1",
    "request": "2.42.0"
  },
  "description": "Crawler is a web spider written with Nodejs. It gives you the full power of jQuery on the server to parse a big number of pages as they are downloaded, asynchronously. Scraping should be simple and fun!",
  "devDependencies": {
    "chai": "1.9.2",
    "jsdom": "3.1.1",
    "mocha": "2.2.1",
    "mocha-testdata": "1.1.0",
    "sinon": "1.11.1"
  },
  "directories": {
    "lib": "lib"
  },
  "dist": {
    "shasum": "15a7f65b68ab2e55613254490c94d7994a4e2c59",
    "tarball": "https://registry.npmjs.org/crawler/-/crawler-0.4.3.tgz"
  },
  "engines": [
    "node >=0.8.x"
  ],
  "gitHead": "587f31a44c5e172a7637923b661dbcc8394b73d7",
  "homepage": "https://github.com/sylvinus/node-crawler",
  "keywords": [
    "dom",
    "javascript",
    "crawling",
    "spider",
    "scraper",
    "scraping",
    "jquery",
    "crawler"
  ],
  "licenses": [
    {
      "type": "MIT",
      "url": "http://github.com/sylvinus/node-crawler/blob/master/LICENSE.txt"
    }
  ],
  "main": "./lib/crawler",
  "maintainers": [
    {
      "email": "sylvain@sylvainzimmer.com",
      "name": "sylvinus"
    },
    {
      "email": "paul.valla+npm@gmail.com",
      "name": "paulvalla"
    }
  ],
  "name": "crawler",
  "optionalDependencies": {
    "iconv": "*"
  },
  "readme": "ERROR: No README data found!",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/sylvinus/node-crawler.git"
  },
  "scripts": {
    "test": "./node_modules/mocha/bin/mocha --reporter spec --bail --timeout 10000 tests/*.js"
  },
  "version": "0.4.3"
}
